{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import auth\n","auth.authenticate_user()\n","import gspread\n","from google.auth import default\n","creds, _ = default()\n","\n","gc = gspread.authorize(creds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RL4ELfZp33qP","executionInfo":{"status":"ok","timestamp":1654762513070,"user_tz":-120,"elapsed":1926,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"outputId":"a05091a9-4860-4426-de81-1eb65691d0f4"},"id":"RL4ELfZp33qP","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pip install torchdata"],"metadata":{"id":"1V1qLQNh4IKg","executionInfo":{"status":"ok","timestamp":1654762515596,"user_tz":-120,"elapsed":2530,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e9c54fb-0e65-467c-c086-36d87c42ce7c"},"id":"1V1qLQNh4IKg","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n","Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.2.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.5.18.1)\n"]}]},{"cell_type":"markdown","source":["## Previous Transformer"],"metadata":{"id":"yFfiTLn4UPu3"},"id":"yFfiTLn4UPu3"},{"cell_type":"code","execution_count":null,"id":"impaired-purpose","metadata":{"id":"impaired-purpose"},"outputs":[],"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","        self.init_weights()\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, src_mask):\n","        src = self.encoder(src) * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"several-brazilian","metadata":{"id":"several-brazilian"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","source":["# Get the data"],"metadata":{"id":"RA9rqKhC4TGI"},"id":"RA9rqKhC4TGI"},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import json\n","import itertools\n"],"metadata":{"id":"PaBe3iLH4Zdo"},"id":"PaBe3iLH4Zdo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_path = '/content/drive/MyDrive/MyProject/Moonboard/Videos/' # To modify with your path\n","my_path = '/content/drive/MyDrive/MyProject/Moonboard/' # To modify with your path\n","move_seq_path = my_path + 'MoveSeqs/'\n","holds_seq_path = my_path + 'HoldsSeqs/'"],"metadata":{"id":"06hXfmDq4Si_"},"id":"06hXfmDq4Si_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["worksheet = pd.read_csv(my_path + 'videos.csv')\n","worksheet = worksheet.iloc[:,1:]\n","worksheet"],"metadata":{"id":"riYjop5R4Wt4","executionInfo":{"status":"ok","timestamp":1654762515600,"user_tz":-120,"elapsed":36,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/","height":677},"outputId":"da527020-4c43-4957-8a6a-fdea3ece23a7"},"id":"riYjop5R4Wt4","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        name\n","0   0000.mp4\n","1   0001.mp4\n","2   0002.mp4\n","3   0003.mp4\n","4   0004.mp4\n","5   0005.mp4\n","6   0006.mp4\n","7   0007.mp4\n","8   0008.mp4\n","9   0009.mp4\n","10  0010.mp4\n","11  0011.mp4\n","12  0012.mp4\n","13  0013.mp4\n","14  0014.mp4\n","15  0015.mp4\n","16  0016.mp4\n","17  0017.mp4\n","18  0018.mp4\n","19  0019.mp4"],"text/html":["\n","  <div id=\"df-b0bf0fd6-1795-41f2-95a5-d5fe0d1fd668\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0001.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0002.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0003.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0004.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0005.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0006.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0007.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0008.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0009.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0010.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0011.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0012.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0013.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0014.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0015.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0016.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0017.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0018.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0019.mp4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0bf0fd6-1795-41f2-95a5-d5fe0d1fd668')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b0bf0fd6-1795-41f2-95a5-d5fe0d1fd668 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b0bf0fd6-1795-41f2-95a5-d5fe0d1fd668');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":424}]},{"cell_type":"markdown","source":["### Test with one sequence"],"metadata":{"id":"9NhH6rtRCPb-"},"id":"9NhH6rtRCPb-"},{"cell_type":"code","source":["video = worksheet.iloc[0,0]\n","target = pd.read_csv(move_seq_path + video + '_MOVE_SEQ.csv').iloc[:,2:]\n","\n","target_coords = target.iloc[:,:2]\n","target_tokens = target.iloc[:,2]"],"metadata":{"id":"T89LpkVQ4f23"},"id":"T89LpkVQ4f23","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# target_coords"],"metadata":{"id":"iiqL93nLMuYl"},"id":"iiqL93nLMuYl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We discretize the coordinates on a grid by rounding them up"],"metadata":{"id":"G8cmGfV9I8d_"},"id":"G8cmGfV9I8d_"},{"cell_type":"code","source":["nb_decimals = 2\n","def standardize_df(df):\n","    return df.round(nb_decimals)"],"metadata":{"id":"vugmV2XkIaqG"},"id":"vugmV2XkIaqG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_coords = standardize_df(target_coords)\n","target_coords"],"metadata":{"id":"8NjO0VsUIsGT","executionInfo":{"status":"ok","timestamp":1654762515602,"user_tz":-120,"elapsed":34,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"e18b27f1-7bb2-4f10-e267-2937aca18128"},"id":"8NjO0VsUIsGT","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      x     y\n","0  0.24  0.63\n","1  0.42  0.57\n","2  0.42  0.40\n","3  0.21  0.66\n","4  0.74  0.32\n","5  0.46  0.57\n","6  0.40  0.41"],"text/html":["\n","  <div id=\"df-7a1310f5-816a-4a3e-9865-85101ec677b9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.24</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.42</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.42</td>\n","      <td>0.40</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.21</td>\n","      <td>0.66</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.74</td>\n","      <td>0.32</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.46</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.40</td>\n","      <td>0.41</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a1310f5-816a-4a3e-9865-85101ec677b9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7a1310f5-816a-4a3e-9865-85101ec677b9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7a1310f5-816a-4a3e-9865-85101ec677b9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":428}]},{"cell_type":"markdown","source":["We define a position vocabulary for the discretized coordinates"],"metadata":{"id":"dpIFxBrrJHmq"},"id":"dpIFxBrrJHmq"},{"cell_type":"code","source":["position_vocabulary = {i/(10**nb_decimals) : i for i in range(10**nb_decimals)}\n","position_vocabulary[-1] = 10**nb_decimals\n","\n","# Convert the coordinates using the above defined tokenizer\n","target_coords[\"x\"].replace(position_vocabulary, inplace = True)\n","target_coords[\"y\"].replace(position_vocabulary, inplace = True)\n","\n","target_coords = target_coords.sort_values(by = ['x'])\n","target_coords.index = range(target_coords.shape[0])\n","target_coords"],"metadata":{"id":"vOMRXxh5JPkZ","executionInfo":{"status":"ok","timestamp":1654762515603,"user_tz":-120,"elapsed":33,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"e1f57d25-fa73-4928-db5d-edf4900f927e"},"id":"vOMRXxh5JPkZ","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      x     y\n","0  21.0  66.0\n","1  24.0  63.0\n","2  40.0  41.0\n","3  42.0  57.0\n","4  42.0  40.0\n","5  46.0  57.0\n","6  74.0  32.0"],"text/html":["\n","  <div id=\"df-fc05958a-136e-4231-898e-6f91ed75b288\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>21.0</td>\n","      <td>66.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>24.0</td>\n","      <td>63.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>40.0</td>\n","      <td>41.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>42.0</td>\n","      <td>57.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>42.0</td>\n","      <td>40.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>46.0</td>\n","      <td>57.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>74.0</td>\n","      <td>32.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc05958a-136e-4231-898e-6f91ed75b288')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fc05958a-136e-4231-898e-6f91ed75b288 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fc05958a-136e-4231-898e-6f91ed75b288');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":429}]},{"cell_type":"markdown","source":["In order to work with this sequence, we need to put it in a tensor"],"metadata":{"id":"AYUw1csDDxwV"},"id":"AYUw1csDDxwV"},{"cell_type":"code","source":["def convert_df_into_tensor(df):\n","    \"\"\"Concatenates all the rows of a dataframe into a big list of strings.\n","    WARNING: the column names are not registered, so the order has to be implicitly respected\"\"\"\n","    df_list = torch.empty(df.shape)\n","\n","    for i in range(df.shape[0]):\n","        df_list[i] = (torch.Tensor(df.iloc[i]))\n","\n","    # df_list = list(itertools.chain.from_iterable(df_list))\n","    \n","    return df_list\n","\n","target_coords_tensor = convert_df_into_tensor(target_coords)"],"metadata":{"id":"uo1sO2c0DxGr"},"id":"uo1sO2c0DxGr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we generate permutations of the target sequence to get the input sequences. We also generate the token sequence (0,1,2...), which will be shuffled accordingly to get the target output sequence. We also generate the trivial token vocabulary"],"metadata":{"id":"eJIQGhBXE51c"},"id":"eJIQGhBXE51c"},{"cell_type":"code","source":["MAX_LENGTH = 20\n","def generate_token_sequences(nb_seqs, length = MAX_LENGTH):\n","  token_sequence = torch.linspace(0,length-1, length, dtype=torch.int64)\n","  return token_sequence.repeat(nb_seqs, 1)\n","\n","token_vocabulary = {f'hold_{i}' : i for i in range(MAX_LENGTH + 1)} # the last one is the additional token that will be used for padding\n","inverse_token_vocabulary = {i: f'hold_{i}' for i in range(MAX_LENGTH + 1)}"],"metadata":{"id":"9P5l84SFPgzJ"},"id":"9P5l84SFPgzJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From the target sequence, generate random holds sequence for input \n","def generate_input_sequences(target, nb_perms = 2):\n","  input_seqs = []\n","  output_seqs = []\n","  input_token_seqs = generate_token_sequences(nb_seqs = nb_perms, length = target.shape[0])\n","  output_token_seqs = []\n","  for n in range(nb_perms):\n","    full_input = torch.cat((target, input_token_seqs[n].view(-1,1)),1)\n","    shuffled_input = np.random.permutation(full_input)\n","\n","    input_seqs.append((torch.Tensor(shuffled_input[:,:2])))\n","    output_token_seqs.append(torch.Tensor(shuffled_input[:,2:]).view(-1))\n","    output_seqs.append(target)\n","    \n","\n","  return input_seqs, output_seqs, torch.vstack(output_token_seqs)\n","\n","input_seqs, output_seqs, output_token_seqs = generate_input_sequences(target_coords_tensor)"],"metadata":{"id":"G1u1SaoH9czb"},"id":"G1u1SaoH9czb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_seqs"],"metadata":{"id":"tLI04l18PdL8","executionInfo":{"status":"ok","timestamp":1654762516004,"user_tz":-120,"elapsed":27,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"419a62bd-5820-4f0b-f1cc-1bcb55279a8a"},"id":"tLI04l18PdL8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[24., 63.],\n","         [42., 57.],\n","         [21., 66.],\n","         [74., 32.],\n","         [40., 41.],\n","         [46., 57.],\n","         [42., 40.]]), tensor([[46., 57.],\n","         [24., 63.],\n","         [74., 32.],\n","         [42., 40.],\n","         [40., 41.],\n","         [42., 57.],\n","         [21., 66.]])]"]},"metadata":{},"execution_count":433}]},{"cell_type":"markdown","source":["Finally, we have to pad the sequences"],"metadata":{"id":"JGMRrhfqHFpI"},"id":"JGMRrhfqHFpI"},{"cell_type":"code","source":["# We define a function to generate a dummy input sequence of length MAX_LENGTH\n","def generate_dummy_sequence(dummy_char = -1):\n","  return torch.full((MAX_LENGTH, 2), fill_value = dummy_char)\n","\n","dummy_seq = generate_dummy_sequence()\n","outputs = output_seqs + [dummy_seq]\n","inputs = input_seqs + [dummy_seq]"],"metadata":{"id":"AjI0I8T5IVhU"},"id":"AjI0I8T5IVhU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we use this function to pad the coordinates sequences\n","inputs_coords = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=position_vocabulary[-1])\n","outputs_coords = torch.nn.utils.rnn.pad_sequence(outputs, batch_first=True, padding_value=position_vocabulary[-1])"],"metadata":{"id":"Qvj42rJzHy8O"},"id":"Qvj42rJzHy8O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To get the full input token sequences, we just generate them with length = MAX_LENGTH\n","inputs_token_seqs = generate_token_sequences(nb_seqs = inputs_coords.shape[0] - 1, length = MAX_LENGTH)\n","inputs_token_seqs[:,output_token_seqs.shape[1]:] = MAX_LENGTH\n","# To get the full target token sequences, we take the input ones and replace the first part (non padded) by the target sequences previously generated\n","# output_token_seqs = torch.where()"],"metadata":{"id":"FWvVaRWLCV-X"},"id":"FWvVaRWLCV-X","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now have 4 list:\n","- output_seqs (nb_perms, len(sequence), 2) containing the coordinates and the limbs of the target holds sequence, ordered, repeated nb_perms times\n","- input_seqs (nb_perms, len(sequence), 2), same, but the nb_perms objects are permuted version of the target sequence\n","- input_token_seqs (nb_perms, len(sequence)) containing the numbers from 0 to len(sequence) - 1, representing the holds in the input sequence\n","- output_token_seqs (nb_perms, len(sequence)), same, but they are permuted in the same way as the coordinates, so that we have the actual target sequence to look for"],"metadata":{"id":"r3UaMoP_F2QQ"},"id":"r3UaMoP_F2QQ"},{"cell_type":"code","source":["print(inputs_token_seqs.shape)"],"metadata":{"id":"n3tSHx5FFxRL","executionInfo":{"status":"ok","timestamp":1654762516006,"user_tz":-120,"elapsed":25,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0789572d-d7ff-4872-a399-eb599b78480a"},"id":"n3tSHx5FFxRL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 20])\n"]}]},{"cell_type":"code","source":["outputs_token_seqs = inputs_token_seqs\n","outputs_token_seqs[:,:output_token_seqs.shape[1]] = output_token_seqs\n","print(outputs_token_seqs.shape)"],"metadata":{"id":"tqbI7mv9F0Hq","executionInfo":{"status":"ok","timestamp":1654762516006,"user_tz":-120,"elapsed":22,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05988bcf-ecc4-41d4-a81b-8ef48c06ee41"},"id":"tqbI7mv9F0Hq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 20])\n"]}]},{"cell_type":"markdown","source":["Now, in order to work (?), the Transformer should complete the sequences by looking at a concatenation of the inputs and outputs. We do so by obtaining 2 sequences of length 39, one made of the 20 inputs and first 19 outputs, this will be our INPUT. The other is made of the last 19 inputs and the 20 outputs, this is our OUTPUT."],"metadata":{"id":"PhtaQOW5JUnT"},"id":"PhtaQOW5JUnT"},{"cell_type":"code","source":["inputs_tokens_cat = torch.cat((inputs_token_seqs, outputs_token_seqs[:,:-1]), dim=1)\n","outputs_tokens_cat = torch.cat((inputs_token_seqs[:,1:], outputs_token_seqs), dim=1)\n","inputs_coords_cat = torch.cat((inputs_coords, outputs_coords[:,:-1]), dim=1)\n","outputs_coords_cat = torch.cat((inputs_coords[:,1:], outputs_coords), dim=1)"],"metadata":{"id":"Ryy9sfQGBt_-"},"id":"Ryy9sfQGBt_-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_coords_cat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaBDRWzhNCcU","executionInfo":{"status":"ok","timestamp":1654762516007,"user_tz":-120,"elapsed":19,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"outputId":"52d02253-f256-4d2f-8d82-30057f911f49"},"id":"TaBDRWzhNCcU","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 24.,  63.],\n","         [ 42.,  57.],\n","         [ 21.,  66.],\n","         [ 74.,  32.],\n","         [ 40.,  41.],\n","         [ 46.,  57.],\n","         [ 42.,  40.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [ 21.,  66.],\n","         [ 24.,  63.],\n","         [ 40.,  41.],\n","         [ 42.,  57.],\n","         [ 42.,  40.],\n","         [ 46.,  57.],\n","         [ 74.,  32.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.]],\n","\n","        [[ 46.,  57.],\n","         [ 24.,  63.],\n","         [ 74.,  32.],\n","         [ 42.,  40.],\n","         [ 40.,  41.],\n","         [ 42.,  57.],\n","         [ 21.,  66.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [ 21.,  66.],\n","         [ 24.,  63.],\n","         [ 40.,  41.],\n","         [ 42.,  57.],\n","         [ 42.,  40.],\n","         [ 46.,  57.],\n","         [ 74.,  32.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.],\n","         [100., 100.]],\n","\n","        [[ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.],\n","         [ -1.,  -1.]]])"]},"metadata":{},"execution_count":440}]},{"cell_type":"markdown","source":["### Now that we have all of these functions, we can process the entire dataframe by reading the data for all sequences."],"metadata":{"id":"fGNYxOCOQ0CB"},"id":"fGNYxOCOQ0CB"},{"cell_type":"code","source":["def prepare_data(sheet, nb_perms = 30):\n","  \"\"\"Returns 2 lists of arrays containing all sequences of holds and moves for each climb\"\"\"\n","  input_seqs = []\n","  output_seqs = []\n","  output_token_seqs = []\n","\n","  inputs_tokens = torch.Tensor()\n","  outputs_tokens = torch.Tensor()\n","\n","  for i in range(sheet.shape[0]):\n","    if i%10 == 0:\n","      print(f'Preparing data for video {i}/{sheet.shape[0] - 1}')\n","    video = sheet.iloc[i,0]\n","    try:\n","      target = pd.read_csv(move_seq_path + video + '_MOVE_SEQ.csv').iloc[:,2:]\n","\n","      target_coords = target.iloc[:,:2]\n","      target_tokens = target.iloc[:,2]\n","    except FileNotFoundError:\n","      continue\n","\n","    if(target_coords.shape[0] > 4):\n","      try:\n","        target_coords = standardize_df(target_coords)\n","        target_coords[\"x\"].replace(position_vocabulary, inplace = True)\n","        target_coords[\"y\"].replace(position_vocabulary, inplace = True)\n","\n","        # We want to test if the model is able to capture the positional embbeding so we test if it can order by x coordinates\n","        # target_coords = target_coords.sort_values(by = ['x'])\n","        # target_coords.index = range(target_coords.shape[0])\n","\n","        target_coords_tensor = convert_df_into_tensor(target_coords)\n","        \n","        input_seqs_video, output_seqs_video, output_token_seqs_video = generate_input_sequences(target_coords_tensor, nb_perms=nb_perms)\n","        input_seqs += input_seqs_video\n","        output_seqs += output_seqs_video\n","\n","        # Copy the shuffled token sequences and add them to the list\n","        input_token_seqs = generate_token_sequences(nb_perms)\n","        input_token_seqs[:,output_token_seqs_video.shape[1]:] = MAX_LENGTH # the last ones are padded with the additional token\n","        outputs_token_seqs = torch.clone(input_token_seqs)\n","        outputs_token_seqs[:,:output_token_seqs_video.shape[1]] = output_token_seqs_video\n","\n","        inputs_tokens_cat = torch.cat((input_token_seqs, outputs_token_seqs[:,:-1]), dim=1)\n","        outputs_tokens_cat = torch.cat((input_token_seqs[:,1:], outputs_token_seqs), dim=1)\n","        \n","\n","        inputs_tokens = torch.cat((inputs_tokens, inputs_tokens_cat))\n","        outputs_tokens = torch.cat((outputs_tokens, outputs_tokens_cat))\n","\n","      except TypeError:\n","          continue\n","\n","  # Pad the sequences\n","  dummy_seq = generate_dummy_sequence()\n","  outputs = output_seqs + [dummy_seq]\n","  inputs = input_seqs + [dummy_seq]\n","\n","  inputs_coords = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=position_vocabulary[-1])\n","  outputs_coords = torch.nn.utils.rnn.pad_sequence(outputs, batch_first=True, padding_value=position_vocabulary[-1])\n","\n","\n","  # Remove the dummy sequence\n","  nb_seqs = inputs_coords.shape[0] - 1\n","  inputs_coords = inputs_coords[:nb_seqs]\n","  outputs_coords = outputs_coords[:nb_seqs]\n","\n","\n","  inputs_coords_cat = torch.cat((inputs_coords, outputs_coords[:,:-1]), dim=1)\n","  outputs_coords_cat = torch.cat((inputs_coords[:,1:], outputs_coords), dim=1)\n","\n","  return inputs_coords_cat, outputs_coords_cat, inputs_tokens, outputs_tokens\n","\n","inputs_coords, outputs_coords, inputs_tokens, outputs_tokens = prepare_data(worksheet, nb_perms=50)\n","print(f'Prepared sequences of shape {inputs_coords.shape}')"],"metadata":{"id":"FKtekkZkQzUL","executionInfo":{"status":"ok","timestamp":1654762516236,"user_tz":-120,"elapsed":245,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"63c59b10-06a6-4f69-bd51-aea8a0eccf63"},"id":"FKtekkZkQzUL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing data for video 0/19\n","Preparing data for video 10/19\n","Prepared sequences of shape torch.Size([1000, 39, 2])\n"]}]},{"cell_type":"code","source":["print(inputs_coords.shape)\n","print(outputs_coords.shape)\n","print(inputs_tokens.shape)\n","print(outputs_tokens.shape)"],"metadata":{"id":"9Q0oxPWYN07z","executionInfo":{"status":"ok","timestamp":1654762516237,"user_tz":-120,"elapsed":27,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95bcb019-d2af-41d0-bb41-6d1ceb670da0"},"id":"9Q0oxPWYN07z","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1000, 39, 2])\n","torch.Size([1000, 39, 2])\n","torch.Size([1000, 39])\n","torch.Size([1000, 39])\n"]}]},{"cell_type":"code","source":["inputs_coords[1]"],"metadata":{"id":"xB7gy7NI6Zja","executionInfo":{"status":"ok","timestamp":1654762516238,"user_tz":-120,"elapsed":24,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5f022fa-40f5-4646-95ce-4e17835f175b"},"id":"xB7gy7NI6Zja","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 42.,  57.],\n","        [ 24.,  63.],\n","        [ 40.,  41.],\n","        [ 42.,  40.],\n","        [ 21.,  66.],\n","        [ 46.,  57.],\n","        [ 74.,  32.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [ 24.,  63.],\n","        [ 42.,  57.],\n","        [ 42.,  40.],\n","        [ 21.,  66.],\n","        [ 74.,  32.],\n","        [ 46.,  57.],\n","        [ 40.,  41.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.],\n","        [100., 100.]])"]},"metadata":{},"execution_count":443}]},{"cell_type":"code","source":["inputs_tokens[1]"],"metadata":{"id":"M_losO9z6g1Z","executionInfo":{"status":"ok","timestamp":1654762516239,"user_tz":-120,"elapsed":22,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ecb4b955-0f2f-4e9e-f9dc-8ea6854979a4"},"id":"M_losO9z6g1Z","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6., 20., 20., 20., 20., 20., 20., 20.,\n","        20., 20., 20., 20., 20., 20.,  1.,  0.,  6.,  2.,  3.,  5.,  4., 20.,\n","        20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.])"]},"metadata":{},"execution_count":444}]},{"cell_type":"code","source":["outputs_tokens[1]"],"metadata":{"id":"X07PBd5j7R8m","executionInfo":{"status":"ok","timestamp":1654762516239,"user_tz":-120,"elapsed":18,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"98ac7193-4f0b-48bf-a509-3ce95053b83a"},"id":"X07PBd5j7R8m","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1.,  2.,  3.,  4.,  5.,  6., 20., 20., 20., 20., 20., 20., 20., 20.,\n","        20., 20., 20., 20., 20.,  1.,  0.,  6.,  2.,  3.,  5.,  4., 20., 20.,\n","        20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.])"]},"metadata":{},"execution_count":445}]},{"cell_type":"markdown","source":["Finally we concatenate all the data to create the input and target tensors"],"metadata":{"id":"tDgpGJPQWIhe"},"id":"tDgpGJPQWIhe"},{"cell_type":"code","source":["sequence_length = 2 * MAX_LENGTH - 1\n","input = torch.cat((inputs_coords, inputs_tokens.view(-1, sequence_length, 1)), 2)\n","target = outputs_tokens.view(-1)"],"metadata":{"id":"DfvGjt2gV-uH"},"id":"DfvGjt2gV-uH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input.shape)\n","print(target.shape)"],"metadata":{"id":"XAxk7iyaXc2B","executionInfo":{"status":"ok","timestamp":1654762516241,"user_tz":-120,"elapsed":16,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8ddc398-2348-4ce1-d6f5-b9a787ad2008"},"id":"XAxk7iyaXc2B","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1000, 39, 3])\n","torch.Size([39000])\n"]}]},{"cell_type":"markdown","source":["# At that point, we have all the input and output sequences prepared, both for coordinates and tokens. We now have to feed them into the Transformer."],"metadata":{"id":"rRkgb1m9WpyI"},"id":"rRkgb1m9WpyI"},{"cell_type":"markdown","source":["First, we adapt the position embedding to work with our data format"],"metadata":{"id":"28hM85bKVJ9A"},"id":"28hM85bKVJ9A"},{"cell_type":"code","source":["class PositionalEncoding_modified(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len = 5000):\n","        super(PositionalEncoding_modified, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.d_model = d_model\n","\n","\n","    def pe(self, position):\n","        pe = torch.zeros(position.shape[0], position.shape[1], self.d_model).to(device)\n","        div_term = (torch.exp(torch.arange(0, self.d_model).float() * (-math.log(10000.0) / self.d_model))).view(1,-1).repeat(2,1).to(device)\n","\n","        # full_position_x = torch.arange(0, self.d_model, dtype=torch.float).view(1,1,-1).repeat(position.shape[0],position.shape[1],1)\n","        # full_position_y = torch.arange(0, self.d_model, dtype=torch.float).view(1,1,-1).repeat(position.shape[0],position.shape[1],1)\n","\n","        # print(full_position_x.shape, position.shape)\n","        # full_position_x[:, :, :position.shape[1]] = position[:,:,0]\n","        # full_position_y[:, :, :position.shape[1]] = position[:,:,1]\n","\n","        # pe[:, :, 0::2] = torch.sin(position[:,:,0::2] * div_term) # Embed x\n","        # pe[:, :, 1::2] = torch.cos(position[:,:,1::2] * div_term) # Embed y\n","\n","        # Multiply by div_term for later\n","        position_scaled = position @ div_term\n","\n","        # pe_x = torch.zeros(position.shape[0], position.shape[1], self.d_model).to(device)\n","        # pe_y = torch.zeros(position.shape[0], position.shape[1], self.d_model).to(device)\n","\n","        # # Embed x\n","        # pe_x[:, 0::2, :] = torch.sin(position_scaled[:,0::2,0]) \n","        # pe_x[:, 1::2, :] = torch.cos(position_scaled[:,0::2,0]) \n","\n","        # # Embed y\n","        # pe_y[:, :, 0::2] = torch.sin(position[:,:,1::2] * div_term) \n","        # pe_y[:, :, 1::2] = torch.cos(position[:,:,1::2] * div_term) \n","\n","        # pe = pe_x + pe_y\n","        pe[:, :, 0::2] = torch.sin(position_scaled[:,:,0::2]) \n","        pe[:, :, 1::2] = torch.cos(position_scaled[:,:,1::2])\n","\n","        return pe\n","\n","\n","    def forward(self, x_encoded, coords):\n","        pos_emb = self.pe(coords)\n","        y = x_encoded + pos_emb\n","        return self.dropout(y)"],"metadata":{"id":"xQwR4-5dVWCA"},"id":"xQwR4-5dVWCA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test the position embedding with no token embedding for now (keeping the 30 holds sequence)\n","# x = input[0].view(1,39,3).to(device)\n","# print(x.shape)\n","# coords = x[:,:,:2]\n","# print(coords)\n","# print(coords[:,0::2,0])\n","# pos_model = PositionalEncoding_modified(MAX_LENGTH)\n","# pos_emb = pos_model(x[:,:,2], x[:,:,:2])\n","# pos_emb"],"metadata":{"id":"QijyqCKdViZm"},"id":"QijyqCKdViZm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerModel_modified(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel_modified, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding_modified(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","        self.init_weights()\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, src_mask):\n","        src_encoded = self.encoder(src[:,:,2].int()) * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src_encoded, src[:,:,:2])\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output"],"metadata":{"id":"6CWAn2cBbSpK"},"id":"6CWAn2cBbSpK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actual use of the transformer"],"metadata":{"id":"zOp-Q8nCVW6e"},"id":"zOp-Q8nCVW6e"},{"cell_type":"code","source":["train_per, val_per = 0.6, 0.2\n","train_size = int(train_per * input.shape[0])\n","target_train_size= train_size * input.shape[1]\n","val_size = int(val_per * input.shape[0])\n","target_val_size= val_size * input.shape[1]\n","\n","device = torch.device(\"cpu\")\n","train_data = input[:train_size].to(device)\n","train_target = target[:target_train_size].to(device)\n","val_data = input[train_size: train_size + val_size].to(device)\n","val_target = target[target_train_size: target_train_size + target_val_size].to(device)\n","test_data = input[train_size + val_size:].to(device)\n","test_target = target[target_train_size + target_val_size:].to(device)"],"metadata":{"id":"PXo_vQzrYlKo"},"id":"PXo_vQzrYlKo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["bptt = 100\n","def get_batch(input_source, target_source, i):\n","    i_target = input_source.shape[1] * i\n","    target_seq_len = min(bptt*input_source.shape[1], len(target_source) - input_source.shape[1] - i_target)\n","    seq_len = min(bptt, len(input_source) - 1 - i)\n","    data = input_source[i:i+seq_len]\n","    target = target_source[i_target+1:i_target+1+target_seq_len].long()\n","    return data, target\n","\n","data, targets = get_batch(train_data, train_target,  100)\n","print(data.shape, targets.shape)"],"metadata":{"id":"FsU8H1rNY8jS","executionInfo":{"status":"ok","timestamp":1654762516429,"user_tz":-120,"elapsed":7,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f4b919b-0624-4973-88b4-81fdf395287b"},"id":"FsU8H1rNY8jS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([100, 39, 3]) torch.Size([3900])\n"]}]},{"cell_type":"code","source":["ntokens = MAX_LENGTH + 1# the size of vocabulary\n","emsize = 512 # embedding dimension\n","nhid = 512 # the dimension of the feedforward network model in nn.TransformerEncoder\n","nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","nhead = 2 # the number of heads in the multiheadattention models\n","dropout = 0.2 # the dropout value\n","model = TransformerModel_modified(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"],"metadata":{"id":"oq8YbRSfYCeE"},"id":"oq8YbRSfYCeE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["bptt = 100\n","# Just a test to check the whole Transformer's application\n","src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n","src, targets = get_batch(train_data, train_target, 0)\n","print(\"data \", data.shape)\n","src_encoded = model.encoder(src[:,:,2].int()) * math.sqrt(model.ninp)\n","print(\"encoded \", src_encoded.shape)\n","src = model.pos_encoder(src_encoded, src[:,:,:2])\n","output = model.transformer_encoder(src, src_mask)\n","output = model.decoder(output)"],"metadata":{"id":"VITHeczTYXub","executionInfo":{"status":"ok","timestamp":1654762517635,"user_tz":-120,"elapsed":1211,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bc14c31-6d09-4ee6-a28c-bfbeae19d52b"},"id":"VITHeczTYXub","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data  torch.Size([100, 39, 3])\n","encoded  torch.Size([100, 39, 512])\n"]}]},{"cell_type":"code","execution_count":null,"id":"durable-transport","metadata":{"id":"durable-transport"},"outputs":[],"source":["import time\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = 4.5*10**-4 # learning rate\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.96), eps=10**(-8), weight_decay=4.5**-2)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","def train():\n","    model.train() # Turn on the train mode\n","    total_loss = 0.\n","    start_time = time.time()\n","    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, train_target, i)\n","        optimizer.zero_grad()\n","        if data.size(0) != bptt:\n","            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n","        output = model(data, src_mask)\n","        loss = criterion(output.view(-1, ntokens), targets)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = 1\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  'lr {:02.2f} | ms/batch {:5.2f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n","                    elapsed * 1000 / log_interval,\n","                    cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(eval_model, data_source, target_source):\n","    eval_model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, bptt):\n","            data, targets = get_batch(data_source, target_source, i)\n","            if data.size(0) != bptt:\n","                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n","            output = eval_model(data, src_mask)\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += len(data) * criterion(output_flat, targets).item()\n","    return total_loss / (len(data_source) - 1)"]},{"cell_type":"code","execution_count":null,"id":"virgin-muscle","metadata":{"id":"virgin-muscle","executionInfo":{"status":"ok","timestamp":1654762698603,"user_tz":-120,"elapsed":180974,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49611765-8cbf-4e3c-8cc5-ec1ec9792fe8"},"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |     1/    6 batches | lr 0.00 | ms/batch 8066.26 | loss  7.81 | ppl  2461.30\n","| epoch   1 |     2/    6 batches | lr 0.00 | ms/batch 4644.97 | loss  1.98 | ppl     7.22\n","| epoch   1 |     3/    6 batches | lr 0.00 | ms/batch 6234.58 | loss  2.05 | ppl     7.78\n","| epoch   1 |     4/    6 batches | lr 0.00 | ms/batch 3268.27 | loss  1.66 | ppl     5.29\n","| epoch   1 |     5/    6 batches | lr 0.00 | ms/batch 2584.64 | loss  1.47 | ppl     4.34\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 26.47s | valid loss  2.35 | valid ppl    10.51\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |     1/    6 batches | lr 0.00 | ms/batch 5140.36 | loss  2.93 | ppl    18.70\n","| epoch   2 |     2/    6 batches | lr 0.00 | ms/batch 2576.58 | loss  1.42 | ppl     4.15\n","| epoch   2 |     3/    6 batches | lr 0.00 | ms/batch 2578.49 | loss  1.58 | ppl     4.88\n","| epoch   2 |     4/    6 batches | lr 0.00 | ms/batch 2590.02 | loss  1.49 | ppl     4.44\n","| epoch   2 |     5/    6 batches | lr 0.00 | ms/batch 2542.64 | loss  1.20 | ppl     3.33\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 17.09s | valid loss  2.13 | valid ppl     8.42\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |     1/    6 batches | lr 0.00 | ms/batch 5107.69 | loss  2.71 | ppl    14.96\n","| epoch   3 |     2/    6 batches | lr 0.00 | ms/batch 2545.98 | loss  1.21 | ppl     3.37\n","| epoch   3 |     3/    6 batches | lr 0.00 | ms/batch 2611.66 | loss  1.49 | ppl     4.42\n","| epoch   3 |     4/    6 batches | lr 0.00 | ms/batch 2562.00 | loss  1.30 | ppl     3.67\n","| epoch   3 |     5/    6 batches | lr 0.00 | ms/batch 2532.37 | loss  1.29 | ppl     3.64\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 16.97s | valid loss  2.07 | valid ppl     7.89\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |     1/    6 batches | lr 0.00 | ms/batch 5018.77 | loss  2.63 | ppl    13.81\n","| epoch   4 |     2/    6 batches | lr 0.00 | ms/batch 2562.26 | loss  1.12 | ppl     3.05\n","| epoch   4 |     3/    6 batches | lr 0.00 | ms/batch 2566.81 | loss  1.29 | ppl     3.62\n","| epoch   4 |     4/    6 batches | lr 0.00 | ms/batch 2598.71 | loss  1.25 | ppl     3.47\n","| epoch   4 |     5/    6 batches | lr 0.00 | ms/batch 2616.44 | loss  1.12 | ppl     3.08\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 17.03s | valid loss  1.92 | valid ppl     6.83\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |     1/    6 batches | lr 0.00 | ms/batch 5200.77 | loss  2.39 | ppl    10.96\n","| epoch   5 |     2/    6 batches | lr 0.00 | ms/batch 2549.06 | loss  1.04 | ppl     2.83\n","| epoch   5 |     3/    6 batches | lr 0.00 | ms/batch 2551.28 | loss  1.25 | ppl     3.50\n","| epoch   5 |     4/    6 batches | lr 0.00 | ms/batch 2556.17 | loss  1.17 | ppl     3.21\n","| epoch   5 |     5/    6 batches | lr 0.00 | ms/batch 2551.18 | loss  1.12 | ppl     3.07\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 17.05s | valid loss  1.91 | valid ppl     6.77\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |     1/    6 batches | lr 0.00 | ms/batch 5190.72 | loss  2.33 | ppl    10.27\n","| epoch   6 |     2/    6 batches | lr 0.00 | ms/batch 2566.98 | loss  1.05 | ppl     2.85\n","| epoch   6 |     3/    6 batches | lr 0.00 | ms/batch 2566.68 | loss  1.17 | ppl     3.23\n","| epoch   6 |     4/    6 batches | lr 0.00 | ms/batch 2591.00 | loss  1.17 | ppl     3.22\n","| epoch   6 |     5/    6 batches | lr 0.00 | ms/batch 2565.60 | loss  1.05 | ppl     2.85\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time: 17.17s | valid loss  1.90 | valid ppl     6.72\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |     1/    6 batches | lr 0.00 | ms/batch 5107.42 | loss  2.18 | ppl     8.84\n","| epoch   7 |     2/    6 batches | lr 0.00 | ms/batch 3643.63 | loss  1.04 | ppl     2.83\n","| epoch   7 |     3/    6 batches | lr 0.00 | ms/batch 2511.52 | loss  1.16 | ppl     3.20\n","| epoch   7 |     4/    6 batches | lr 0.00 | ms/batch 2544.88 | loss  1.10 | ppl     3.00\n","| epoch   7 |     5/    6 batches | lr 0.00 | ms/batch 2546.73 | loss  1.05 | ppl     2.86\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | time: 18.00s | valid loss  1.91 | valid ppl     6.75\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |     1/    6 batches | lr 0.00 | ms/batch 5143.38 | loss  2.17 | ppl     8.72\n","| epoch   8 |     2/    6 batches | lr 0.00 | ms/batch 2564.39 | loss  1.01 | ppl     2.76\n","| epoch   8 |     3/    6 batches | lr 0.00 | ms/batch 2535.69 | loss  1.15 | ppl     3.15\n","| epoch   8 |     4/    6 batches | lr 0.00 | ms/batch 2542.96 | loss  1.05 | ppl     2.87\n","| epoch   8 |     5/    6 batches | lr 0.00 | ms/batch 2518.43 | loss  1.02 | ppl     2.76\n","-----------------------------------------------------------------------------------------\n","| end of epoch   8 | time: 16.96s | valid loss  1.99 | valid ppl     7.35\n","-----------------------------------------------------------------------------------------\n","| epoch   9 |     1/    6 batches | lr 0.00 | ms/batch 5153.91 | loss  2.09 | ppl     8.12\n","| epoch   9 |     2/    6 batches | lr 0.00 | ms/batch 2562.69 | loss  1.00 | ppl     2.72\n","| epoch   9 |     3/    6 batches | lr 0.00 | ms/batch 2611.06 | loss  1.15 | ppl     3.15\n","| epoch   9 |     4/    6 batches | lr 0.00 | ms/batch 2591.49 | loss  1.07 | ppl     2.91\n","| epoch   9 |     5/    6 batches | lr 0.00 | ms/batch 2522.29 | loss  0.99 | ppl     2.69\n","-----------------------------------------------------------------------------------------\n","| end of epoch   9 | time: 17.10s | valid loss  2.11 | valid ppl     8.28\n","-----------------------------------------------------------------------------------------\n","| epoch  10 |     1/    6 batches | lr 0.00 | ms/batch 5046.75 | loss  2.08 | ppl     8.04\n","| epoch  10 |     2/    6 batches | lr 0.00 | ms/batch 2595.68 | loss  1.00 | ppl     2.73\n","| epoch  10 |     3/    6 batches | lr 0.00 | ms/batch 2594.84 | loss  1.11 | ppl     3.02\n","| epoch  10 |     4/    6 batches | lr 0.00 | ms/batch 2589.74 | loss  1.04 | ppl     2.84\n","| epoch  10 |     5/    6 batches | lr 0.00 | ms/batch 2558.47 | loss  1.02 | ppl     2.76\n","-----------------------------------------------------------------------------------------\n","| end of epoch  10 | time: 17.06s | valid loss  2.13 | valid ppl     8.39\n","-----------------------------------------------------------------------------------------\n"]}],"source":["best_val_loss = float(\"inf\")\n","epochs = 10 # The number of epochs\n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train()\n","    val_loss = evaluate(model, val_data, val_target)\n","    print('-' * 89)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                     val_loss, math.exp(val_loss)))\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"id":"atlantic-abortion","metadata":{"id":"atlantic-abortion","executionInfo":{"status":"ok","timestamp":1654762700413,"user_tz":-120,"elapsed":1835,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0688732-fa77-417c-b23f-5be361bf195f"},"outputs":[{"output_type":"stream","name":"stdout","text":["=========================================================================================\n","| End of training | test loss  1.84 | test ppl     6.29\n","=========================================================================================\n"]}],"source":["test_loss = evaluate(best_model, test_data, test_target)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"]},{"cell_type":"code","source":["# Just to stop the execution if execute all\n","# br"],"metadata":{"id":"FDc9pjvfcAHt"},"id":"FDc9pjvfcAHt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tensor_to_tokens(my_tensor):\n","  \"\"\"Converts the output tensor holding the holds embedding to the holds word\"\"\"\n","  x = [int(t) for t in my_tensor]\n","  return [inverse_token_vocabulary[t] for t in x]"],"metadata":{"id":"DAxiF9IU5h0u"},"id":"DAxiF9IU5h0u","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ultimate-saver","metadata":{"id":"ultimate-saver"},"outputs":[],"source":["def write_sentence(xx):\n","    sentence = \"\"\n","    for word in tensor_to_tokens(xx.reshape(-1)):\n","        sentence+= word +\" \"\n","    print(sentence)\n","    \n","def complete_sentence(xx, length, src_mask):\n","    sentence = \"\"\n","    # print(xx)\n","    # for word in tensor_to_tokens(xx[0,:,2]):\n","    #     sentence+= word +\" \"\n","    sentence += \"|\"\n","    # crete new tokens\n","    for i in range(length):\n","        # Compute output of the model from the current sequence\n","        out = model(xx.to(device), src_mask)[0]\n","        # Take the label with max probability\n","        labels = torch.argmax(out, 1).view(-1)\n","        # Extract the corresponding token and append it to the sequence\n","        new_token = tensor_to_tokens(labels.reshape(-1))[-1]\n","        sentence += new_token + \" \"\n","\n","        # Now we want to update the current sequence by appending the information \n","        # corresponding to the computed token\n","\n","        # Compute the index of the corresponding token in the input sequence\n","        index_xx_hold = torch.argwhere(xx[:,:,2] == labels.reshape(-1)[-1:].item())[0][-1]\n","        prediction = xx[:,index_xx_hold,:].reshape(1,1,3)\n","        xx = torch.cat((xx, prediction),1)\n","        src_mask = model.generate_square_subsequent_mask(len(xx)).to(device)\n","    \n","    print(sentence[sentence.find('|'):])\n","\n","    return sentence\n","    \n","    \n","# t = \"At the time of his marriage, William's father, John Yeats, was studying law, but would later pursue art studies at Heatherley School of Fine Art, in London. William's mother, Susan Mary Pollexfen, came from Sligo, from a wealthy merchant family, which owned a\"\n","# t = torch.tensor(vocab(tokenizer(t)))\n","# src_mask = model.generate_square_subsequent_mask(len(t)).to(device)\n","# t = t.reshape([-1, 1]).to(device)\n","\n","# complete_sentence(t, 100, src_mask)  "]},{"cell_type":"code","execution_count":null,"id":"understood-storm","metadata":{"id":"understood-storm"},"outputs":[],"source":["idx_to_check = 869\n","t = input[idx_to_check].view(1, input.shape[1],-1)\n","t_input = t.clone()\n","# t_input[:,MAX_LENGTH:,:] = t_input[:,0,:].repeat(1,MAX_LENGTH-1,1)\n","# print(t_input)"]},{"cell_type":"code","source":["src_mask = model.generate_square_subsequent_mask(len(t_input)).to(device)\n","t_input = t_input.to(device)\n","\n","sentence = complete_sentence(t_input, 60, src_mask) "],"metadata":{"id":"3b5x4-426nKA","executionInfo":{"status":"ok","timestamp":1654762701740,"user_tz":-120,"elapsed":1330,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"497d81b0-1cc5-468e-ae56-d8d31530273a"},"id":"3b5x4-426nKA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 hold_20 \n"]}]},{"cell_type":"code","source":["outputs_tokens[idx_to_check]"],"metadata":{"id":"2mJA6Bi7UtQZ","executionInfo":{"status":"ok","timestamp":1654762701741,"user_tz":-120,"elapsed":14,"user":{"displayName":"Thomas Rimbot","userId":"00803464136383640136"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"288a1a73-d8b6-4c96-b9d6-59fc1efcdb79"},"id":"2mJA6Bi7UtQZ","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 20., 20., 20., 20., 20.,\n","        20., 20., 20., 20., 20.,  5.,  9.,  4.,  0.,  2.,  6.,  8.,  7.,  1.,\n","         3., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.])"]},"metadata":{},"execution_count":463}]},{"cell_type":"code","source":[""],"metadata":{"id":"e3KL2sibUujl"},"id":"e3KL2sibUujl","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Transformer_diagnosis.ipynb","provenance":[],"collapsed_sections":["yFfiTLn4UPu3"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}